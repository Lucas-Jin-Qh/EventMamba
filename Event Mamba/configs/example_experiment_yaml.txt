# Example Experiment Configuration for EventMamba
# This file demonstrates how to create custom experiment configurations
# by overriding default settings

# Experiment metadata
experiment:
  name: "eventmamba_hqf_baseline"
  description: "Baseline EventMamba training on HQF dataset"
  tags: ["baseline", "hqf", "full_resolution"]

# Override general settings
general:
  seed: 2024
  device: cuda:0
  mixed_precision: true
  log_interval: 50

# Override training settings
training:
  num_epochs: 300  # Fewer epochs for this experiment
  batch_size: 8    # Larger batch size if GPU memory allows
  patch_size: 224  # Slightly smaller patches
  learning_rate: 2e-4  # Higher initial learning rate
  
  # Custom optimizer settings
  optimizer:
    type: AdamW
    betas: [0.9, 0.95]  # More aggressive beta2
    weight_decay: 0.05  # Stronger weight decay
    
  # Custom scheduler
  scheduler:
    type: CosineAnnealingLR
    T_max: 300
    eta_min: 1e-7
    
  # Specific augmentation settings
  augmentation:
    random_flip: true
    random_crop: true
    random_time_flip: false  # Disable time flip for this experiment
    random_noise: true
    noise_std: 0.05  # Less noise
    drop_event_prob: 0.05  # Lower drop probability

# Override model settings
model:
  name: EventMamba_v1
  
  # Use smaller model for faster training
  unet:
    base_channels: 24  # Fewer channels
    num_stages: 4
    channel_multiplier: [1, 2, 3, 4]  # Less aggressive scaling
    
rwo_mamba:
  window_size: 16  # Larger windows
  monte_carlo:
    num_samples: 4  # Fewer MC samples for speed

hsfc_mamba:
  curve_order: 2  # Lower order Hilbert curve
  
ssm:
  state_dim: 8  # Smaller state dimension

# Override loss settings
loss:
  type: combined
  l1_weight: 10.0  # Less weight on L1
  lpips_weight: 5.0  # More weight on perceptual loss
  temporal_consistency_weight: 0.0  # Disable for this experiment

# Dataset-specific overrides
dataset:
  train:
    event_path: /data/HQF/train/events
    frame_path: /data/HQF/train/frames
    split_file: /data/HQF/splits/train.txt
    
  val:
    event_path: /data/HQF/val/events
    frame_path: /data/HQF/val/frames
    split_file: /data/HQF/splits/val.txt
    
  test:
    event_path: /data/HQF/test/events
    frame_path: /data/HQF/test/frames
    split_file: /data/HQF/splits/test.txt

# Event representation overrides
event_representation:
  num_bins: 3  # Fewer temporal bins
  normalize: true

# Preprocessing overrides
preprocessing:
  time_window: 33.0  # milliseconds (30 fps equivalent)
  filter_hot_pixels: true
  hot_pixel_threshold: 200  # More aggressive filtering

# Logging settings for this experiment
logging:
  use_tensorboard: true
  use_wandb: true
  project_name: EventMamba_Experiments
  experiment_name: hqf_baseline_v1
  log_dir: ./logs/hqf_baseline
  save_dir: ./checkpoints/hqf_baseline

# Validation settings
validation:
  batch_size: 1
  save_visualization: true
  num_vis_samples: 10  # Save more visualizations

# Hardware optimization for this experiment
optimization:
  use_amp: true
  gradient_checkpointing: true  # Enable to save memory
  channels_last: true

# Custom settings for different phases
phase_specific:
  # Warmup phase (first 10 epochs)
  warmup:
    learning_rate: 1e-5
    augmentation:
      random_noise: false
      drop_event_prob: 0.0
      
  # Fine-tuning phase (last 50 epochs)
  finetune:
    learning_rate: 1e-5
    batch_size: 4
    monte_carlo:
      num_samples: 8  # More samples for better quality

# Ablation study settings (commented out)
# ablation:
#   disable_rwo: false
#   disable_hsfc: false
#   disable_bidirectional: false
#   use_standard_conv: false

# Multi-GPU settings (if needed)
# distributed:
#   enabled: true
#   world_size: 4
#   backend: nccl

# Notes for this experiment
notes: |
  This is a baseline experiment for EventMamba on the HQF dataset.
  Key changes from default:
  - Smaller model (24 base channels instead of 32)
  - Higher learning rate with cosine annealing
  - More aggressive perceptual loss weight
  - No temporal consistency loss
  - Gradient checkpointing enabled for memory efficiency