# EventMamba Default Configuration
# This file contains all default configurations for training, validation, and testing

# General settings
general:
  seed: 42
  device: cuda
  num_workers: 8
  pin_memory: true
  mixed_precision: true
  gradient_clip: 1.0
  log_interval: 100
  checkpoint_interval: 5
  validation_interval: 1

# Training settings
training:
  num_epochs: 400
  batch_size: 4
  patch_size: 256
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_epochs: 10
  accumulation_steps: 1
  
  # Optimizer settings
  optimizer:
    type: AdamW
    betas: [0.9, 0.999]
    eps: 1e-8
    
  # Learning rate scheduler
  scheduler:
    type: ExponentialLR
    gamma: 0.99
    min_lr: 1e-6
    
  # Data augmentation
  augmentation:
    random_flip: true
    random_crop: true
    random_time_flip: true
    random_noise: true
    noise_std: 0.1
    drop_event_prob: 0.1
    
# Validation settings
validation:
  batch_size: 1
  save_visualization: true
  num_vis_samples: 5
  metrics:
    - mse
    - ssim
    - lpips
    - temporal_consistency

# Testing settings
testing:
  batch_size: 1
  monte_carlo_samples: 8
  save_predictions: true
  save_metrics: true
  test_time_augmentation: false

# Loss function settings
loss:
  type: combined
  l1_weight: 20.0
  lpips_weight: 2.0
  temporal_consistency_weight: 0.5
  lpips_net: vgg

# Logging settings
logging:
  use_tensorboard: true
  use_wandb: false
  project_name: EventMamba
  experiment_name: null  # Will be auto-generated if null
  log_dir: ./logs
  save_dir: ./checkpoints

# Distributed training settings
distributed:
  enabled: false
  backend: nccl
  world_size: 1
  find_unused_parameters: false

# Hardware optimization
optimization:
  use_amp: true  # Automatic Mixed Precision
  channels_last: true  # Memory format optimization
  cudnn_benchmark: true
  gradient_checkpointing: false